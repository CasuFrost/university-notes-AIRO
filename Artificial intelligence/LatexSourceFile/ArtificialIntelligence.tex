\documentclass[10pt, letterpaper]{report}
% !TeX program = xelatex
%==================PREAMBOLO=======================%
\input{../../preamble/preamble.tex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\titolo}{Artificial intelligence }

 %TOGLI COMMENTO SE USI XELATEX
%\usepackage{fontspec}
\title{\titolo} %========TITOLO========%
\author{Marco Casu}
\date{\vspace{-5ex}}
\begin{document}

%==================COPERTINA=======================%
\begin{titlepage}
    
\begin{center}
    %TOGLI COMMENTO SE USI XELATEX
   %\setmainfont{Palace Script MT}
   \HUGE Marco Casu\acc
\end{center}
\thispagestyle{empty}
\begin{figure}[h]
    \centering{
        %l'immagine deve avere una risoluzione 2048x2048
        \includegraphics[width=1\textwidth ]{images/Copertina.png}
    }
\end{figure}
\vfill 
\centering \includegraphics[width=0.4\textwidth ]{../../preamble/Stemma_sapienza.png} \acc
\centering \Large \color{sapienza}Faculty of Information Engineering, Computer Science and Statistics\\
Department of Computer, Control and Management Engineering\\
Master's degree in Artificial Intelligence and Robotics
\end{titlepage}

%===================FINE COPERTINA======================%
\newpage
%\pagecolor{cartaRiciclata}%\setmainfont{Algerian}
\Large
This document summarizes and presents the topics for the \titolo course for the Master's degree in Artificial Intelligence and Robotics at Sapienza University of Rome. The document is free for any use. If the reader notices any typos, they are kindly requested to report them to the author.
\vfill
\begin{figure}[h!]
    \raggedright
    \includegraphics[width=0.4\textwidth,right ]{../../preamble/tomodachi.pdf} 
\end{figure}
\newpage %\setmainfont{Times New Roman}
\normalsize

\tableofcontents 
\newpage

%==================FOOTER e HEADER=======================%
\fancyhf{}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{Sezione \thesection}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{\titolo}
\fancyfoot[R]{ Marco Casu}
%\fancyfoot[R]{\setmainfont{Palace Script MT}\huge Marco Casu \setmainfont{Times New Roman}}
%==================FOOTER e HEADER=======================%
\newtheorem{definition}{Definition}
%==================INIZIO======================%
\chapter{Introduzione}
\section{Basic Definitions}
In the context of the artificial intelligence, an \textbf{agent} is an entity that can\begin{itemize}
    \item Perceive the environment through \textit{sensors} (percepts)
    \item Act upon the environment through \textit{actuators} (actions).
\end{itemize}
We say that an agent is \textbf{rational} if he selects the action that maximize a given \textit{performance measure}, informally, he attempts to do ''the right thing''. The best case is hypothetical and often unattainable, because the agent usually can't perform all the actions needed, and can't perceive all the information about the environment.\bigskip

An agent has a performance measure $M$ and a set $A$ of all possible actions, given percept a sequence $A$ and knowledge $K$ (data), he has to select the next action $a\in A$, is a map\begin{equation}
    M\times P\times K \longrightarrow A.
\end{equation} 
An action $a$ is optimal if it maximize the expected value of $M$, given the sequence $P$ and the knowledge $K$. An agent is rational if he always chooses the optimal action. More specifically, an agent consists in two components:\begin{itemize}
    \item an architecture which provides an interface to the environment
    \item a program executed on that architecture.
\end{itemize}
There are some limitation that we aren't considering, such as the fact that determining the optimal choice could take too much time or memory on the architecture.

\subsection{Types of Agents}
There are different kinds of agents, a \textbf{Table Driven Agent} is the simplest form of agent architecture. It's essentially a look-up table that maps every possible sequence of percepts (what the agent has sensed so far) to a corresponding action the agent should take. His behavior can be resumed in the algorithm \ref{alg:table_agent}.

\begin{algorithm}
    \caption{Table Driven Agent}\label{alg:table_agent}
    \begin{algorithmic}
    \Require \textit{percepts}
    \State \textbf{persistent}: \textit{percepts}, a sequence, initially empty
    \State\hphantom{persistent: .} \textit{table}, a table of actions, indexed by percept sequences, initially fully specified
    \State append \textit{percept} to the end of \textit{percepts} 
    \State \textit{action}$\leftarrow$\texttt{LookUp}(\textit{percepts,table})
    \State\Return \textit{action}
    \end{algorithmic}
\end{algorithm}
\bigskip


A \textbf{Reflex Agent} consists in three components:\begin{itemize}
    \item sensors to get information from the environment
    \item a decision making process, in form of a \textit{condition-action rules}, typically looks like \texttt{IF (condition) THEN (action)}.
    \item actuators, the outputs that allow the agent to affect or change the environment.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/reflexAgent.png}
    \caption{Reflex agent diagram}
\end{figure}\bigskip

A \textbf{Model-Based Reflex Agent} is an enhanced version of the previous one, the key enhancement here is the inclusion of an \textit{Internal State} and a \textit{Model of the World} to make up for the agent's limited view of the environment. The internal state cannot simply be the last thing the agent saw; it needs to be updated to reflect reality. This is done using a Model of the World, which contains two key pieces of knowledge:\begin{itemize}
    \item 
    How the world evolves independently of the agent, his accounts for changes in the environment that occur regardless of the agent's actions (e.g., a clock ticking, an external event).
    \item
    How the agent's own actions affect the world, this is the effect of the agent's previous action (e.g., if the agent drove forward, its position changed).
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/ModelreflexAgent.png}
    \caption{Model Based Reflex agent diagram}
\end{figure}\bigskip

If a model based reflex agent consider the future prospective, is a \textbf{Goal Based Agent}, as shown in figure \ref{img:goal_agent}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/goal.png}
    \caption{Goal Based agent diagram}
    \label{img:goal_agent}
    \includegraphics[width=0.4\textwidth ]{images/utility.png}
    \caption{Utility Based agent diagram}
\end{figure}\bigskip

A \textbf{Utility Based Agent} is equipped with a \textit{utility function} that maps a state to a number which represents how
desirable the state is. Agentâ€™s utility function is an internalization of the performance function.\bigskip

A \textbf{Learning Agent} is an architecture designed to improve its efficiency over time by separating four functions:\begin{itemize}
    \item  the performance element selects actions\item  
     the critic provides feedback on those actions against a standard\item   the learning element uses this feedback to update the agent's internal knowledge\item   the problem generator suggests exploratory actions to gain new knowledge. This structure enables the agent to continuously adapt and improve its decision-making.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/learn_agent.png}
    \caption{Learning agent diagram}
\end{figure}\bigskip

An agent can be classified in one of the following groups:\begin{itemize}
    \item a \textbf{domain specific agent }  is a solver specific to a particular problem (such as playing chess), is usually more efficient.
    \item a \textbf{general agent} is a solver for general problems, such as learning the rule of any board game, is usually more intelligent but less efficient.
\end{itemize}
\subsection{The Environment}
An environment can be classified in terms of different attributes:\begin{itemize}
    \item An environment can be \textbf{fully observable} if all the relevant information are accessible to the sensors, otherwise is \textbf{partially observable}.
    \item If there are no uncertainty, the environment is \textbf{deterministic}. An environment is \textbf{stochastic} if uncertainty is quantified by using probabilities, otherwise is \textbf{non deterministic} if uncertainty is managed as actions with multiple outcomes.
    \item An environment is \textbf{episodic} if the correctness of an action can be evaluated instantly, otherwise if are evaluated in the future developments, is \textbf{sequential}. 
    \item An environment can be \textbf{static} or \textbf{dynamic}, if itdoes not change, but the agent's performance
score changes, the environment is called \textbf{semi-dynamic}.
    \item An environment can be perceived as \textbf{discrete} or \textbf{continuous}.
    \item In a single environment there may be multiple agent, that can be \textbf{competitive} or \textbf{cooperative}.
\end{itemize}
Many sub-areas of AI can be classified by:\begin{itemize}
    \item Domain-specific vs. general.
    \item The environment.
    \item Particular agent architectures sometimes also play a role, especially
    in Robotics.
\end{itemize}
It follows a classification of some areas in terms of the attributes we discussed:\begin{itemize}
    \item \textbf{Classical Search}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item sequential
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{domain specific}.
    \item \textbf{Planning}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static
        \item sequential 
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{general}.
    \item \textbf{Adversarial Search}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item sequential    
        \item discrete
        \item multi-agent
    \end{itemize}
    and the approach is \textit{domain specific}.
    \item \textbf{General Game Playing}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item sequential    
        \item discrete
        \item multi-agent
    \end{itemize}
    and the approach is \textit{general}.
    \item \textbf{Constraint Satisfaction \& Reasoning}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item episodic    
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{general}.
    \item \textbf{Probabilistic Reasoning}, the environment is\begin{itemize}
        \item partially observable
        \item stochastic
        \item static 
        \item episodic    
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{general}.
\end{itemize}
\chapter{Search Problems}
\section{Classical Search}
Let's consider two basic example of classical search problems, the first one is the following:\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/map_search.png}
\end{center}
Starting from Zurigo, we would like to find a route to Zagabria. We have an initial state (Zurigo), and we have to apply actions (drive) to reach the goal state (Zagabria). Another example is the following, we want to solve the tiles-puzzle game, shown in figure \ref{img:tiles}, to reach the left state, starting from the right one, the actions to perform is the move of the tiles. A performance measure could be to minimize the summed-up action costs.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth ]{images/tile.png}
    \caption{The tile game}
    \label{img:tiles}
\end{figure}

In the classical search context, we restrict the agent's environment to a very simple setting, with a finite number of states and actions, a single agent, a fully observable stati environment that doesn't evolve, given that assumption, the classical search problems are the simplest one, despite that, are very important problems in practice.\bigskip

\noindent Every problem specifies a state space.

\begin{definition}
    A \textbf{State Space} is a 6-tuple $\Theta=(S,A,c,T,I,S^G)$ where:\begin{itemize}
        \item $S$ is a finite set of the \textit{states}.
        \item $A$ is a finite set of \textit{actions}.
        \item $c:A\rightarrow\R^+$ is the \textit{cost function}.
        \item $T\subseteq S\times A\times S $ is the \textit{transition relation}, that describes how an action on a given state make the agent evolve to the next state. We assume that the problem is deterministic, so for all $s\in S$, $a\in A$, if $(s,a,s')\in T$ and $(s,a,s'')\in T$ then $s'=s''$.
        \item $I\in S$ is the \textit{initial state}
        \item $S^G\subseteq S$ is the set of the \textit{goal states}, where we want to end.
    \end{itemize}
\end{definition}
A transition $(s,a,s')$ can be denoted $s \xrightarrow{a} s'$, we say that $s\rightarrow s'$ if $\exists a$ such that $(s,a,s')\in T$ . We say that $\Theta$ has \textit{unit costs} if $\forall a\in A, \ \ c(a)=1$. A state space can be illustrated as a directed labeled graph. \begin{definition}
    Let $\Theta=(S,A,c,T,I,S^G)$ to be a state space, we say that\begin{itemize}
        \item $s'$ is a \textbf{successor} of $s$ if $s\rightarrow s'$
        \item $s'$ is a \textbf{predecessor} of $s$ if $s'\rightarrow sT$
        \item we say that $s'$ is \textbf{reachable from} $s$ if\begin{align}
            &\exists (a_1\dots, a_n)\subseteq A\\
            &\exists (s_2\dots, s_{n-1})\subseteq S\\
            &(s,a_1,s_2)\in T\\
            &(s_2,a_2,s_3)\in T\\
            \vdots \\
            &(s_{n-1},a_n,s')\in T 
        \end{align}
        we can write the sequence as follows\begin{equation}
            s\xrightarrow{a_1}s_2,\dots, s_{n-1}\xrightarrow{a_n}s'.
        \end{equation}
        \item We say that $s$ is \textbf{reachable} (without reference state) if is reachable from $I$.
        \item $s$ is \textbf{solvable} if there exists $s'\in S^G$ such that $s'$ is reachable from $s$, otherwise $s$ is \textbf{dead end}.
    \end{itemize}
\end{definition}
\begin{definition}
    Let $\Theta=(S,A,c,T,I,S^G)$ to be a state space, and let $s\in S$. A \textbf{solution} for $s$ is a path from $s$ to some goal state $s'\in S^G$. The solution is \textbf{optimal} if it's cost is minimal, let $H$ to be the set of all possible solution (sequence of states) for $s$\begin{equation}
        H=\{\text{paths from }s\text{ to }s'\in S^G\}=\{(s_{i0},s_{i1},s_{i1},\dots s_{in})  : s_{in}\in S^G, \ s_{i0}=s\}
    \end{equation}
    where $n^i$ is the length of the $i$-th solution.
    The optimal solution is\begin{equation}
        \arg \min_{(s,s_{i1},\dots s_{in})\in H}\sum_{j=0}^{n^i} c(s_{ij}).
    \end{equation}
\end{definition}
A solution for $I$ is called \textbf{solution for $\Theta$}, if such that solution exists, $\Theta$ is \textbf{solvable}.
\end{document}