\documentclass[10pt, letterpaper]{report}
% !TeX program = xelatex
%==================PREAMBOLO=======================%
\input{../../preamble/preamble.tex}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\titolo}{Artificial intelligence }

 %TOGLI COMMENTO SE USI XELATEX
%\usepackage{fontspec}
\title{\titolo} %========TITOLO========%
\author{Marco Casu}
\date{\vspace{-5ex}}
\begin{document}

%==================COPERTINA=======================%
\begin{titlepage}
    
\begin{center}
    %TOGLI COMMENTO SE USI XELATEX
   %\setmainfont{Palace Script MT}
   \HUGE Marco Casu\acc
\end{center}
\thispagestyle{empty}
\begin{figure}[h]
    \centering{
        %l'immagine deve avere una risoluzione 2048x2048
        \includegraphics[width=1\textwidth ]{images/Copertina.png}
    }
\end{figure}
\vfill 
\centering \includegraphics[width=0.4\textwidth ]{../../preamble/Stemma_sapienza.png} \acc
\centering \Large \color{sapienza}Faculty of Information Engineering, Computer Science and Statistics\\
Department of Computer, Control and Management Engineering\\
Master's degree in Artificial Intelligence and Robotics
\end{titlepage}

%===================FINE COPERTINA======================%
\newpage
%\pagecolor{cartaRiciclata}%\setmainfont{Algerian}
\Large
This document summarizes and presents the topics for the \titolo course for the Master's degree in Artificial Intelligence and Robotics at Sapienza University of Rome. The document is free for any use. If the reader notices any typos, they are kindly requested to report them to the author.
\vfill
\begin{figure}[h!]
    \raggedright
    \includegraphics[width=0.4\textwidth,right ]{../../preamble/tomodachi.pdf} 
\end{figure}
\newpage %\setmainfont{Times New Roman}
\normalsize

\tableofcontents 
\newpage

%==================FOOTER e HEADER=======================%
\fancyhf{}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{Sezione \thesection}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{\titolo}
\fancyfoot[R]{ Marco Casu}
%\fancyfoot[R]{\setmainfont{Palace Script MT}\huge Marco Casu \setmainfont{Times New Roman}}
%==================FOOTER e HEADER=======================%
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
%==================INIZIO======================%
\chapter{Introduzione}
\section{Basic Definitions}
In the context of the artificial intelligence, an \textbf{agent} is an entity that can\begin{itemize}
    \item Perceive the environment through \textit{sensors} (percepts)
    \item Act upon the environment through \textit{actuators} (actions).
\end{itemize}
We say that an agent is \textbf{rational} if he selects the action that maximize a given \textit{performance measure}, informally, he attempts to do ''the right thing''. The best case is hypothetical and often unattainable, because the agent usually can't perform all the actions needed, and can't perceive all the information about the environment.\bigskip

An agent has a performance measure $M$ and a set $A$ of all possible actions, given percept a sequence $A$ and knowledge $K$ (data), he has to select the next action $a\in A$, is a map\begin{equation}
    M\times P\times K \longrightarrow A.
\end{equation} 
An action $a$ is optimal if it maximize the expected value of $M$, given the sequence $P$ and the knowledge $K$. An agent is rational if he always chooses the optimal action. More specifically, an agent consists in two components:\begin{itemize}
    \item an architecture which provides an interface to the environment
    \item a program executed on that architecture.
\end{itemize}
There are some limitation that we aren't considering, such as the fact that determining the optimal choice could take too much time or memory on the architecture.

\subsection{Types of Agents}
There are different kinds of agents, a \textbf{Table Driven Agent} is the simplest form of agent architecture. It's essentially a look-up table that maps every possible sequence of percepts (what the agent has sensed so far) to a corresponding action the agent should take. His behavior can be resumed in the algorithm \ref{alg:table_agent}.

\begin{algorithm}
    \caption{Table Driven Agent}\label{alg:table_agent}
    \begin{algorithmic}
    \Require \textit{percepts}
    \State \textbf{persistent}: \textit{percepts}, a sequence, initially empty
    \State\hphantom{persistent: .} \textit{table}, a table of actions, indexed by percept sequences, initially fully specified
    \State append \textit{percept} to the end of \textit{percepts} 
    \State \textit{action}$\leftarrow$\texttt{LookUp}(\textit{percepts,table})
    \State\Return \textit{action}
    \end{algorithmic}
\end{algorithm}
\bigskip


A \textbf{Reflex Agent} consists in three components:\begin{itemize}
    \item sensors to get information from the environment
    \item a decision making process, in form of a \textit{condition-action rules}, typically looks like \texttt{IF (condition) THEN (action)}.
    \item actuators, the outputs that allow the agent to affect or change the environment.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/reflexAgent.png}
    \caption{Reflex agent diagram}
\end{figure}\bigskip

A \textbf{Model-Based Reflex Agent} is an enhanced version of the previous one, the key enhancement here is the inclusion of an \textit{Internal State} and a \textit{Model of the World} to make up for the agent's limited view of the environment. The internal state cannot simply be the last thing the agent saw; it needs to be updated to reflect reality. This is done using a Model of the World, which contains two key pieces of knowledge:\begin{itemize}
    \item 
    How the world evolves independently of the agent, his accounts for changes in the environment that occur regardless of the agent's actions (e.g., a clock ticking, an external event).
    \item
    How the agent's own actions affect the world, this is the effect of the agent's previous action (e.g., if the agent drove forward, its position changed).
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/ModelreflexAgent.png}
    \caption{Model Based Reflex agent diagram}
\end{figure}\bigskip

If a model based reflex agent consider the future prospective, is a \textbf{Goal Based Agent}, as shown in figure \ref{img:goal_agent}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/goal.png}
    \caption{Goal Based agent diagram}
    \label{img:goal_agent}
    \includegraphics[width=0.4\textwidth ]{images/utility.png}
    \caption{Utility Based agent diagram}
\end{figure}\bigskip

A \textbf{Utility Based Agent} is equipped with a \textit{utility function} that maps a state to a number which represents how
desirable the state is. Agent’s utility function is an internalization of the performance function.\bigskip

A \textbf{Learning Agent} is an architecture designed to improve its efficiency over time by separating four functions:\begin{itemize}
    \item  the performance element selects actions\item  
     the critic provides feedback on those actions against a standard\item   the learning element uses this feedback to update the agent's internal knowledge\item   the problem generator suggests exploratory actions to gain new knowledge. This structure enables the agent to continuously adapt and improve its decision-making.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth ]{images/learn_agent.png}
    \caption{Learning agent diagram}
\end{figure}\bigskip

An agent can be classified in one of the following groups:\begin{itemize}
    \item a \textbf{domain specific agent }  is a solver specific to a particular problem (such as playing chess), is usually more efficient.
    \item a \textbf{general agent} is a solver for general problems, such as learning the rule of any board game, is usually more intelligent but less efficient.
\end{itemize}
\subsection{The Environment}
An environment can be classified in terms of different attributes:\begin{itemize}
    \item An environment can be \textbf{fully observable} if all the relevant information are accessible to the sensors, otherwise is \textbf{partially observable}.
    \item If there are no uncertainty, the environment is \textbf{deterministic}. An environment is \textbf{stochastic} if uncertainty is quantified by using probabilities, otherwise is \textbf{non deterministic} if uncertainty is managed as actions with multiple outcomes.
    \item An environment is \textbf{episodic} if the correctness of an action can be evaluated instantly, otherwise if are evaluated in the future developments, is \textbf{sequential}. 
    \item An environment can be \textbf{static} or \textbf{dynamic}, if itdoes not change, but the agent's performance
score changes, the environment is called \textbf{semi-dynamic}.
    \item An environment can be perceived as \textbf{discrete} or \textbf{continuous}.
    \item In a single environment there may be multiple agent, that can be \textbf{competitive} or \textbf{cooperative}.
\end{itemize}
Many sub-areas of AI can be classified by:\begin{itemize}
    \item Domain-specific vs. general.
    \item The environment.
    \item Particular agent architectures sometimes also play a role, especially
    in Robotics.
\end{itemize}
It follows a classification of some areas in terms of the attributes we discussed:\begin{itemize}
    \item \textbf{Classical Search}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item sequential
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{domain specific}.
    \item \textbf{Planning}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static
        \item sequential 
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{general}.
    \item \textbf{Adversarial Search}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item sequential    
        \item discrete
        \item multi-agent
    \end{itemize}
    and the approach is \textit{domain specific}.
    \item \textbf{General Game Playing}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item sequential    
        \item discrete
        \item multi-agent
    \end{itemize}
    and the approach is \textit{general}.
    \item \textbf{Constraint Satisfaction \& Reasoning}, the environment is\begin{itemize}
        \item fully observable
        \item deterministic
        \item static 
        \item episodic    
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{general}.
    \item \textbf{Probabilistic Reasoning}, the environment is\begin{itemize}
        \item partially observable
        \item stochastic
        \item static 
        \item episodic    
        \item discrete
        \item single-agent
    \end{itemize}
    and the approach is \textit{general}.
\end{itemize}
\chapter{Search Problems}
\section{Classical Search}
Let's consider two basic example of classical search problems, the first one is the following:\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/map_search.png}
\end{center}
Starting from Zurigo, we would like to find a route to Zagabria. We have an initial state (Zurigo), and we have to apply actions (drive) to reach the goal state (Zagabria). Another example is the following, we want to solve the tiles-puzzle game, shown in figure \ref{img:tiles}, to reach the left state, starting from the right one, the actions to perform is the move of the tiles. A performance measure could be to minimize the summed-up action costs.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth ]{images/tile.png}
    \caption{The tile game}
    \label{img:tiles}
\end{figure}

In the classical search context, we restrict the agent's environment to a very simple setting, with a finite number of states and actions, a single agent, a fully observable stati environment that doesn't evolve, given that assumption, the classical search problems are the simplest one, despite that, are very important problems in practice.\bigskip

\noindent Every problem specifies a state space.

\begin{definition}
    A \textbf{State Space} is a 6-tuple $\Theta=(S,A,c,T,I,S^G)$ where:\begin{itemize}
        \item $S$ is a finite set of the \textit{states}.
        \item $A$ is a finite set of \textit{actions}.
        \item $c:A\rightarrow\R^+$ is the \textit{cost function}.
        \item $T\subseteq S\times A\times S $ is the \textit{transition relation}, that describes how an action on a given state make the agent evolve to the next state. We assume that the problem is deterministic, so for all $s\in S$, $a\in A$, if $(s,a,s')\in T$ and $(s,a,s'')\in T$ then $s'=s''$.
        \item $I\in S$ is the \textit{initial state}
        \item $S^G\subseteq S$ is the set of the \textit{goal states}, where we want to end.
    \end{itemize}
\end{definition}
A transition $(s,a,s')$ can be denoted $s \xrightarrow{a} s'$, we say that $s\rightarrow s'$ if $\exists a$ such that $(s,a,s')\in T$ . We say that $\Theta$ has \textit{unit costs} if $\forall a\in A, \ \ c(a)=1$. A state space can be illustrated as a directed labeled graph. \begin{definition}
    Let $\Theta=(S,A,c,T,I,S^G)$ to be a state space, we say that\begin{itemize}
        \item $s'$ is a \textbf{successor} of $s$ if $s\rightarrow s'$
        \item $s'$ is a \textbf{predecessor} of $s$ if $s'\rightarrow sT$
        \item we say that $s'$ is \textbf{reachable from} $s$ if\begin{align}
            &\exists (a_1\dots, a_n)\subseteq A\\
            &\exists (s_2\dots, s_{n-1})\subseteq S\\
            &(s,a_1,s_2)\in T\\
            &(s_2,a_2,s_3)\in T\\
            \vdots \\
            &(s_{n-1},a_n,s')\in T 
        \end{align}
        we can write the sequence as follows\begin{equation}
            s\xrightarrow{a_1}s_2,\dots, s_{n-1}\xrightarrow{a_n}s'.
        \end{equation}
        \item We say that $s$ is \textbf{reachable} (without reference state) if is reachable from $I$.
        \item $s$ is \textbf{solvable} if there exists $s'\in S^G$ such that $s'$ is reachable from $s$, otherwise $s$ is \textbf{dead end}.
    \end{itemize}
\end{definition}
\begin{definition}
    Let $\Theta=(S,A,c,T,I,S^G)$ to be a state space, and let $s\in S$. A \textbf{solution} for $s$ is a path from $s$ to some goal state $s'\in S^G$. The solution is \textbf{optimal} if it's cost is minimal, let $H$ to be the set of all possible solution (sequence of states) for $s$\begin{equation}
        H=\{\text{paths from }s\text{ to }s'\in S^G\}=\{(s_{i0},s_{i1},s_{i1},\dots s_{in})  : s_{in}\in S^G, \ s_{i0}=s\}
    \end{equation}
    where $n^i$ is the length of the $i$-th solution.
    The optimal solution is\begin{equation}
        \arg \min_{(s,s_{i1},\dots s_{in})\in H}\sum_{j=0}^{n^i} c(s_{ij}).
    \end{equation}
\end{definition}
A solution for $I$ is called \textbf{solution for $\Theta$}, if such that solution exists, $\Theta$ is \textbf{solvable}.
\subsection{Vacuum Cleaner Example}
Let's consider a vacuum cleaner, that is the agent of our problem, the goal is to clean a room, the vacuum cleaner can be in two possible points (left and right), this points can be clean or dirty. The agent can perform the following actions\begin{itemize}
    \item move right 
    \item move left 
    \item suck the dust on the floor
\end{itemize}
there are 8 possible states\begin{itemize}
    \item left point clean, right point clean, vacuum cleaner is on right point
    \item left point dirty, right point clean, vacuum cleaner is on right point
    \item left point clean, right point dirty, vacuum cleaner is on right point
    \item left point dirty, right point dirty, vacuum cleaner is on right point
    \item left point clean, right point clean, vacuum cleaner is on left point
    \item left point dirty, right point clean, vacuum cleaner is on left point
    \item left point clean, right point dirty, vacuum cleaner is on left point
    \item left point dirty, right point dirty, vacuum cleaner is on left point
\end{itemize}
the initial state is the one with the left point dirty, right point dirty, and the vacuum cleaner on the left point, we denote the actions $R$ (move right), $L$ (move left), $S$ suck. The state space of the problem is show in figure \ref{img:vacuum_cleaner}. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth ]{images/vacuum.png}
    \caption{The vacuum cleaner state space}
    \label{img:vacuum_cleaner}
\end{figure}
Some example of set of actions that can lead from the initial state to a goal state are\begin{align*}
    &S\longrightarrow R\longrightarrow S\\
    &S\longrightarrow R\longrightarrow R\longrightarrow S\\
    &R\longrightarrow S\longrightarrow L\longrightarrow S
\end{align*}

Typically, the state space is exponentially large in the size of its specification, search problems are typically computationally hard and/or $\mathsf{NP}$-complete. We say that we can give an \textit{explicit description} of a search problem if we can define his state space as a graph.
\section{Problem Descriptions}
\begin{definition}
    We have a \textbf{black box description} of the problem if we can't describe the state space explicitly but we can\begin{itemize}
        \item Know which is the initial state
        \item Check if a given state is a goal state
        \item Check the cost of a given action $a$
        \item Given a state $s$, check all the actions that are applicable to state $s$
        \item Given a state $s$ and an applicable action $a$, we can get the successor state.
    \end{itemize}
\end{definition}
We can think about it in a programming-way, given a problem described by $\Theta=(S,A,c,T,I,S^G)$, we can't check directly $\Theta$, but we have an \textit{API} of the problem that provide the following functions \begin{itemize}
    \item \texttt{InitialState()} : return the initial state of the problem
    \item \texttt{GoalTest($s$)} : return true if and only if $s\in S^G$
    \item \texttt{Cost}($a$ : return $c(a)$)
    \item \texttt{Actions}($s$) : return the set $\{ a \ : \ \exists  s\xrightarrow{a} s'\in T \text{ for some }s'\in S\}$
    \item \texttt{ChildState}($s,a$) : return $s'$ if $s\xrightarrow{a} s'\in T$.
\end{itemize}
We \textbf{specify} a search problem if we can program/access to such an \textit{API}. There are a declarative description too.
\begin{definition}
\end{definition}
We have a \textbf{declarative description} of the problem if is described by the following sets:\begin{itemize}
    \item $P$ is a set of boolean variables (\textit{propositions})
    \item $I\subseteq P$ is the subset of $P$ indicating which propositions are true in the initial states.
    \item $G\subset P$ is the subset of $P$ describing the goal states in the following way\begin{itemize}
        \item a state $s$ is a set of propositions 
        \item $s\subseteq G \iff s$ is a goal state
    \end{itemize}
    \item $A$ is a set of actions, each action $a$ is described by\begin{itemize}
        \item a set $pre_a\subseteq P$ of \textit{precondition}, $a$ can be performed if and only if the conditions in $pre_a$ are true.
        \item a set of propositions $add_a$
        \item a set of propositions $del_a$
        \item the outcome of each action is the state $(\{s\}\cup add_a )\backslash del_a$
        \item $c:A\rightarrow\R$ is the cost function.
    \end{itemize}
\end{itemize}
Declarative descriptions are strictly more powerful than black box ones. In this section we assume the black box description. In principle, the search strategies we will discuss can be used with any
problem description that allows to implement the black box \textit{API}.
\subsection{Missionaries and Cannibals Example}
The problem is the following\begin{itemize}
    \item there are a river and a boat that can carry the people from the left bank to the right 
    \item there are 6 people, 3 missionaries and 3 cannibals 
    \item the boat can carry 0, 1 or 2 people at the same time, not 3 
    \item the goal is to get  everybody to the left bank
    \item if at any time, there are more cannibals than missionaries in one bank, the missionaries get killed and the game is lost. 
\end{itemize}
We can model the problem as follows\begin{itemize}
    \item the state space $S$ is \begin{equation}
        S=\{(M,C,B) \ : \ M+C=6, \ 0\le M \le 3, \ 0\le C \le 3, \  B\in\{0,1\}\}
    \end{equation}
    $M$ represents the current number of missionaries on the right bank, $S$ represents the current number of cannibals on the right bank, $B=1$ if the boat is on the right bank, otherwise is on the left.
    \item the initial state is $(3,3,1)$
    \item the goal states are $S^G={(0,0,0),(0,0,1)}$
    \item each actions have the same cost, is negligible
    \item the action that can be performed are the following\begin{itemize}
        \item if $B=1$, we can subtract (in total) 1 or 2 from $M$ or $C$ (or both), and set $B=0$.
        \item if $B=0$, we can add (in total) 1 or 2 from $M$ or $C$ (or both), and set $B=1$.
    \end{itemize}
    an action is applicable if and only if the following condition are satisfied after\begin{itemize}
        \item $M\ge C$ if $M>0$, this decode the facts that the cannibals can't be more or equals than the missionaries on the right bank if there are missionaries on the left bank
        \item if $M<3$ (some missionaries are on the left bank) then $(3-M)>(3-C)$ the missionaries on the left bank must be greater then the cannibals on the left bank.
    \end{itemize}
\end{itemize}
To search a solution we can start from the initial state and expand the tree of possible solutions accordingly to the applicable actions.\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/missionaries.eps}
\end{center}
\subsection{Tree and Graph Search}
In the search context the following terminology is used
\begin{itemize}
    \item Search node $n$: Contains a state reached by the search, plus information about how it was reached.
    \item Path cost $g(n)$: The cost of the path reaching $n$.
    \item Optimal cost $g^*$: The cost of an optimal solution path. For a state $s$, $g^*(s)$ is the cost of a cheapest path reaching $s$.
    \item Node expansion: Generating all successors of a node, by applying all actions applicable to the node's state $s$. Afterwards, the state $s$ itself is also said to be expanded.
    \item Search strategy: Method for deciding which node is expanded next.
    \item Open list: Set of all nodes that currently are candidates for expansion. Also called frontier.
    \item Closed list: Set of all states that were already expanded. Used only in graph search, not in tree search (up next). Also called explored set.
\end{itemize}
When we explore the state space of a problem we can maintain a closed list of all the node that has been already searched, to check for each generated new node if is already in the list (if so, we discard it). If such list is used, the search is called \textbf{graph search}, else, if the same state may appear in many search nodes, is called \textbf{tree search}. The tree search doesn't use a list so require less memory.\bigskip

When we analyze a search algorithm, we are interested in various properties\begin{itemize}
    \item \textbf{Completeness}: the algorithm is guaranteed to find a solution (it there are one).
    \item \textbf{Optimality}: the returned solution is guaranteed to be optimal. 
    \item \textbf{Time Complexity}: How long does it take to find a solution? (Measured
in generated states).
    \item \textbf{Space Complexity}: How much memory does the search require?
(Measured in states).
    \item \textbf{Branching Factor}: The number $b$ of how many successor a state may have.
    \item \textbf{Gal depth}: the number $d$ of action required to reach the shallowest (nearest to the initial state) goal state.
\end{itemize}
\section{Blind Search}
We talk about \textit{blind search} if the problem does not require any input beyond the problem API. Does not require any additional work from the programmer. For each node $n$ in the search context, we define the following data structure:\begin{itemize}
    \item $n$.State is the state which te node contains 
    \item $n$.Parent is node in the search tree that generated this node
    \item $n$.Action is the action that was applied to the parent to generate the node
    \item $n$.PathCost, also denoted $g(n)$, is the cost of the path from the initial state to the node (as indicated by the parent pointers).
\end{itemize}
On a node we can perform the following operations\begin{itemize}
    \item Solution($n$) returns the path from the initials state to $n$
    \item ChildNode($n,a$) returns the node $n$ corresponding to the application of action $a$ in state $n$.State. 
\end{itemize}
We also have the open list (called frontier) where we can perform the following actions:\begin{itemize}
    \item Empty?(frontier) returns true if and only if there are no more elements in the
open list.
    \item Pop(frontier) returns the first element of the open list, and
removes that element from the list.
    \item Insert(element,frontier) inserts an element into the open list.
\end{itemize}
The insert function can put the element in front, in the last positions, or in other positions, it depends from the implementation (different implementations yield different search strategies).
\subsection{Breadth-First Search}
The strategy is to expand nodes in the order they were produced, as a FIFO queue, we expand the shallowest unexpanded node.
\begin{center}
    \includegraphics[width=1\textwidth ]{images/bfs.png}
\end{center}
This algorithm \ref{alg:BFS} is complete and optimal (in case of unit cost function). We are using the black box \textit{API}.

\begin{algorithm}
    \caption{Breadth-First Search}\label{alg:BFS}
    \begin{algorithmic}
    \Require $problem$
    \State $node\leftarrow$\texttt{InitialState()}
    \If{\texttt{GoalTest($node$.State)}}
    \State\Return Solution($node$)
    \EndIf
    \State $frontier\leftarrow$ a FIFO queue with $node$ in it
    \State $explored\leftarrow$ an empty set
    \While{true}
    \If{Empty?($frontier$)}
    \State\Return Failure
    \EndIf
    \State $node\leftarrow$pop($frontier$)
    \State add $node$.State in $explored$
    \For{each $action$ in \texttt{Actions($node$.State)}}
    \State $child\leftarrow$ChildNode($node,action$)
    \If{$child$.State is not in $explored$ or $frontier$}
    \If{\texttt{GoalTest($child$.State)}}
    \State\Return Solution($child$)
    \EndIf
    \State $frontier\leftarrow$Insert($child$,$frontier$)
    \EndIf
    \EndFor
    \EndWhile
    \end{algorithmic}
\end{algorithm}
Let $b$ to be the maximum branching factor and $d$ the depth of the shallowest goal state, an upper bound (in the worst case) for the number of nodes generated (time complexity) is\begin{equation}
    b+b^2+b^3\dots +b^d\in O(b^d)
\end{equation}
the same for the space complexity since  all generated nodes are kept in memory. Let's see an example, assume that $b=10$, the agent can generate $10^4$ nodes per second, and each node has a size of $1$ kilobyte, we have the following data:\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Depth} & \textbf{Nodes} & \textbf{Time} & \textbf{Memory} \\
        \hline
        2 & 110 & 0.11 milliseconds & 107 kilobytes \\
        \hline
        4 & 11,110 & 11 milliseconds & 10.6 megabytes \\
        \hline
        6 & $10^6$ & 1.1 seconds & 1 gigabyte \\
        \hline
        8 & $10^8$ & 2 minutes & 103 gigabytes \\
        \hline
        10 & $10^{10}$ & 3 hours & 10 terabytes \\
        \hline
        12 & $10^{12}$ & 13 days & 1 petabyte \\
        \hline
        14 & $10^{14}$ & 3.5 years & 99 petabytes \\
        \hline
    \end{tabular}
\end{center}
The critical resource for this method is the memory.
\subsection{Depth-First Search}
The strategy is to expand the most recent explored node, as a LIFO queue, we expand the deepest unexpanded node. 
\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/dfs.png}
\end{center}
The algorithm is not complete since it may take infinite time, since there is no check for cycles along the branches. It's not optimal since he chooses a direction and looks for a path to a goal state. Is typically implemented as a recursive function, as in algorithm \ref{alg:DFS}.\bigskip

\begin{algorithm}
    \caption{Depth-First Search}\label{alg:DFS}
    \begin{algorithmic}
    \Require $problem$, a node $n$
    \If{\texttt{GoalTest}($n$.\texttt{State})}
    \State\Return empty action sequence
    \EndIf
    \For{each $action$ in \texttt{Actions}($node$.\texttt{State})}
    \State $n'\leftarrow$ChildNode($node$,$action$)
    \State result$\leftarrow$Depth-First Search($problem$,$n'$)
    \If{result$\ne$ failure}
    \State\Return $action \ \circ$ result
    \EndIf
    \EndFor
    \State\Return failure
    \end{algorithmic}
\end{algorithm}

\noindent With $action \ \circ$ result is denoted the concatenation of actions. 
About the space complexity, this methods stores only a single path of actions, from the root to a leaf node, since once a
 node has been expanded, it can be removed from memory as soon as all its
 descendants have been fully explored.

 If $m$ is the maximal depth reached, the space occupied is in $O(bm)$. About the time complexity, in the worst case the nodes generated is in $O(b^m)$.
 \subsection{Uniform-Cost Search}
 This methods is equivalent to the well known Dijkstra's algorithm. We expand the node with the lowest path cost $g(n)$, the frontier is ordered by the path cost, with the lowest first. It differs from the BFS since a test is added to check if a better path is found to a node currently
 on the frontier.

 \begin{algorithm}
    \caption{Uniform-Cost Search}\label{alg:DFS}
    \begin{algorithmic}
    \Require $problem$
    \State $node\leftarrow$\texttt{InitialState()}
    \State $frontier\leftarrow$ priority queue ordered by ascending $g$
    \State $explored\leftarrow$ empty set of states
    \While{true}
    \If{Empty?($frontier$)}
    \State\Return failure 
    \EndIf
    \State $n\leftarrow$Pop($frontier$)
    \If{\texttt{GoalTest}($n$.State)}
    \State\Return Solution($n$)
    \EndIf
    $explored\leftarrow explored\cup n$.State
    \For{each $action$ in \texttt{Actions}($node$.\texttt{State})}
    \State $n'\leftarrow${ChildNode}($n$,$a$)
    \If{$n'$.State$\notin explored\cup$States($frontier$)}
    \State $frontier\leftarrow$insert($n'$,$frontier$)
    \Else{ \textbf{if} $\exists n''\in frontier \ : \ n''.$State=$n'$.State$ \land \ g(n')<g(n'')$}
    \State replace $n''$ with $n'$ in $frontier$
    \EndIf
    \EndFor
    \EndWhile
    \end{algorithmic}
\end{algorithm}
\begin{theorem}
    The Uniform-Cost search algorithm is optimal, since the Dijkstra's algorithm is optimal, and Uniform-cost search is equivalent to Dijkstra's algorithm on the state space graph.and 
\end{theorem}

The algorithm is complete if we assume that, the costs are strictly positive and the state space is finite. The time and space complexity are\begin{equation}
    O(b^{1+\lfloor\nicefrac{g^*}{*}\rfloor})
\end{equation}where\begin{itemize}
    \item $g^*$ is the cost of an optimal solution 
    \item $\epsilon=\min c$ is the positive cost of the cheapest action, the minimum of the function $c$.
\end{itemize}
\subsection{Iterative Deepening Search}
This is an altered version of the Depth-First Search algorithm, where we define a predetermined depth limit, and apply the DFS in function of that limit, iteratively applying this by increasing the depth limit each time.
\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/iterative_deep.png}
\end{center}
We split the algorithm in three different function.

 \begin{algorithm}
    \caption{Iterative Deepening Search}\label{alg:IDS}
    \begin{algorithmic}
    \Require $problem$
    \For{$depth=0,1\dots\infty$}
    \State $result\leftarrow$Depth Limited Search($problem$,$depth$)
    \If{$result\ne$cutoff} 
    \State\Return $result$
    \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

 \begin{algorithm}
    \caption{Depth Limited Search}\label{alg:DLS}
    \begin{algorithmic}
    \Require $problem,limit$
    \State $node\leftarrow$ \texttt{InitialState()}
    \State\Return Recursive DLS($node$,$problem$,$limit$)
    \end{algorithmic}
\end{algorithm}

The algorithm is complete since we are keep searching until a solution is found, is also optimal if all the cost are unitary, the space complexity is $O(bd)$. The time complexity is in $O(b^d)$, this methods combines the advantages of breadth-first and depth-first search. It is the preferred blind search method in large state spaces with unknown solution depth.\bigskip

 \begin{algorithm}
    \caption{Recursive DLS}\label{alg:EDLS}
    \begin{algorithmic}
    \Require $n,problem,limit$
    \If{\texttt{GoalTest($n$.State)}}
    \State\Return empty action sequence
    \EndIf
    \If{$limit$==0}
    \State\Return cutoff
    \EndIf
    \State $cutoffOccurred\leftarrow $false
    \For{each $action$ in \texttt{Actions}($node$.\texttt{State})}
    \State $n'\leftarrow$ChildNode($node$,$action$)
    \State result$\leftarrow$Recursive DLS($problem$,$n'$)
    \If{$result$==cutoff}
    \State $cutoffOccurred\leftarrow $true
    \Else
    \If{result$\ne$ failure}
    \State\Return $action \ \circ$ result
    \EndIf
    \EndIf
    \EndFor
    \If{$cutoffOccurred$}
    \State\Return cutoff 
    \EndIf
    \State\Return failure
    \end{algorithmic}
\end{algorithm}

The following table is a summary of the methods that we considered in this section, confronting the time and space complexity.\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
\hline
Criterion                                                  & BFS                                                                 & \begin{tabular}[c]{@{}c@{}}Uniform\\ Cost\end{tabular}                                             & DFS      & \begin{tabular}[c]{@{}c@{}}Depth\\ Limited\end{tabular} & \begin{tabular}[c]{@{}c@{}}Iterative\\ Deeping\end{tabular}         \\ \hline
Completness                                                & \begin{tabular}[c]{@{}c@{}}Yes, if \\ $a$ is finite\end{tabular}    & \begin{tabular}[c]{@{}c@{}}Yes, if \\ $a$ is finite\\ and action costs \\ is positive\end{tabular} & No       & No                                                      & \begin{tabular}[c]{@{}c@{}}Yes, if \\ $a$ is finite\end{tabular}    \\ \hline
Optimality                                                 & \begin{tabular}[c]{@{}c@{}}Yes if action\\ costs are 1\end{tabular} & Yes                                                                                                & No       & No                                                      & \begin{tabular}[c]{@{}c@{}}Yes if action\\ costs are 1\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}Time\\ Complexity\end{tabular}  & $O(b^d)$                                                            & $O(b^{1+\lfloor \nicefrac{g^*}{\epsilon} \rfloor})$                                                & $O(b^m)$ & $O(b^l)$                                                & $O(b^d)$                                                            \\ \hline
\begin{tabular}[c]{@{}c@{}}Space\\ Complexity\end{tabular} & $O(b^d)$                                                            & $O(b^{1+\lfloor \nicefrac{g^*}{\epsilon} \rfloor})$                                                & $O(bm)$  & $O(bl)$                                                 & $O(bd)$                                                             \\ \hline
\end{tabular}
\end{center}
where
\begin{itemize}
\item $b$: finite branching factor
\item $d$: goal depth
\item $m$: maximum depth of the search tree
\item $l$: depth limit
\item $g^*$ : optimal solution cost
\item $\epsilon>0$: minimal action cost.
\end{itemize}
\section{Informed Search}
\redText{TODO}
\section{Local Search}
\redText{TODO}
\section{Adversarial Search}
One of the oldest sub-areas of AI is \textit{Game Planning}, we model games as search problems that takes in account the competition between two opponents (such as chess). We consider simple games that satisfies the following restrictions:\begin{itemize}
    \item the set of game states is discrete 
    \item the number of possible moves at each step is finite 
    \item the game state is fully observable and each move's outcome is deterministic
    \item there are only two players
    \item the players playing in alternating turns 
    \item the utility function $u$ must be maximized from one player and minimized from the other 
    \item there are no infinite runs of the game, after a finite number of steps, the game must end.
\end{itemize}
We consider only zero-sum games, where the two players play with the same conditions, without favor. We denote $Max$ and $Min$ the two players.\begin{definition}
    A \textbf{game state space} is a 6-tuple $\Theta=(S,A,T,I,S^T,u)$ where\end{definition}\begin{itemize}
        \item $S$ is the set of states, can be partitioned in $S=S^{Max}\cup S^{Min}\cup S^T$, denoting the state where $Max$ or $Min$ plays.
        \item $A$ is the set of actions, can be partitioned in $A=A^{Max}\cup A^{Min}$\begin{itemize}
        \item $A^{Max}$ is the set of actions that $Max$ can take, $A^{Min}$ is the set of actions that $Min$ can take. 
            \item for $a\in A^{Max}$, if $s\xrightarrow{a} s'$ then $s\in S^{Max}$ and $s'\in S^{Min}\cup S^T$
            \item for $a\in A^{Min}$, if $s\xrightarrow{a} s'$ then $s\in S^{Min}$ and $s'\in S^{Max}\cup S^T$
        \end{itemize}
        \item $T$ is the set of deterministic transition relation
        \item $I$ is the initial state 
        \item $S^T$ are the set of terminal states
        \item $u:S^T\rightarrow\R$ is the utility function
    \end{itemize}
\begin{definition}
    A \textbf{strategy} for $Max$ is a function $\sigma^{Max}:S^{Max}\rightarrow A^{Max}$ such that, $a$ is applicable to $s$ if $\sigma^{Max}(s)=a$. Analogous for $Min$ with $\sigma^{Min}$.
\end{definition}
$\sigma^{Max}$ (or $\sigma^{Min}$) is defined for all states in $S^{Max}$ (or $S^{Min}$), since the agent don't know how the opponent will react he needs to prepare for all
possibilities. A strategy is optimal if it yields the best possible utility assuming
perfect opponent play. For an adversarial search problem there are three types of solutions:\begin{itemize}
    \item \textbf{ultra weak}: Prove whether the first player will win, lose or draw from the
initial position, given perfect play on both sides.
\item \textbf{weak}: Provide a strategy that is optimal from the beginning of the game
for one player against any possible play by the opponent.
\item\textbf{strong}: Provide a strategy that is optimal from any valid state, even if
imperfect play has already occurred on one or both sides.
\end{itemize}
Computing a strategy is often unfeasible, the number of reachable states are big, in chess, there are $10^{40}$ possibile board states. We will consider a \textit{Black Box} description of the games.
\subsection{Minimax Search}
This is the canonical algorithm to solving games, by computing an optimal strategy. Remember that the player $Max$ attempts to maximize the utility function $u(s)$ of the terminal state that will be reached during play, when $Min$ attempts to minimize it.\bigskip 

We describe the algorithm playing as $Max$, and we assume that the opponents will play by trying always to minimize the utility function. Starting from the actual state, the algorithm explores all the possible outcomes from all possible feasible sequences of moves, expanding a tree.

 \begin{algorithm}
    \caption{Minimax}\label{alg:minimax}
    \begin{algorithmic}
    \Require $s$
    \State $v\leftarrow$MaxValue($s$)
    \State\Return an action $a\in $\texttt{Actions}($s$) yielding value $v$
    \end{algorithmic}
\end{algorithm}
 \begin{algorithm}
    \caption{MaxValue}\label{alg:MaxValue}
    \begin{algorithmic}
    \Require $s$
    \If{TerminalTest($s$)}
    \State\Return $u(s)$
    \EndIf
    \State$v\leftarrow -\infty$
    \For{each $a\in $\texttt{Actions}($s$)}
    \State $v\leftarrow$ max($v$,MinValue(ChildState($s$,$a$)))
    \EndFor 
    \State\Return $v$
    \end{algorithmic}
\end{algorithm}
 \begin{algorithm}
    \caption{MinValue}\label{alg:MinValue}
    \begin{algorithmic}
    \Require $s$
    \If{TerminalTest($s$)}
    \State\Return $u(s)$
    \EndIf
    \State$v\leftarrow +\infty$
    \For{each $a\in $\texttt{Actions}($s$)}
    \State $v\leftarrow$ min($v$,MaxValue(ChildState($s$,$a$)))
    \EndFor 
    \State\Return $v$
    \end{algorithmic}
\end{algorithm}

Consider the tree in figure \ref{fig:minmax}, starting from the root, the player consider all possible outcomes, the leaf are terminating states, assuming that the opponents will always tries to minimize $u$:\begin{itemize}
    \item if $Max$ choose the left branch, $Min$ will choose the action that leads to the terminal state with $u(s)=3$
    \item if $Max$ choose the middle branch, $Min$ will choose the action that leads to the terminal state with $u(s)=2$
    \item if $Max$ choose the right branch, $Min$ will choose the action that leads to the terminal state with $u(s)=2$
\end{itemize}
So the best mooves is the one that leads to the left branch.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth ]{images/minmaxtree.png}
    \caption{Game's tree }
    \label{fig:minmax}
\end{figure}

This is the simplest possible game search algorithm, but in practice is unfeasible since the search tree are way too large to expand. The minimax algorithm is:\begin{itemize}
    \item Complete, if the tree is finite.
    \item Optimal against an optimal opponent. 
\end{itemize}
Since it is a depth first search, the time complexity is $O(b^m)$, where $b$ is the branching factor and $m$ is the depth of the solution. The space complexity is $O(bm)$. In chess, $b\simeq 35$ (possible moves approximately) and a reasonable game terminates in $80$ turns, so $m\simeq 80$, in such a case the state space is $O(35^{80})$ (it's impossible in practice to expand the tree).
\subsection{Evaluation Functions}
Since the Minimax game tree are too big, we impose a depth limit $d$ (called horizon) on the search, and we apply an evaluation function to the non-terminal states in that horizon. An evaluation function $f:S\rightarrow \R$ should work to estimate the actual value reachable from a state such as in the unlimited-depth Minimax. If a state is terminal, we use the actual value $u$. We want $f$ to be accurate and fast to compute.\bigskip 

While applying this algorithm, we can consider a depth limit $d$, evaluate $f$ on all the states at that depth, and then considering these cut-off tree and apply the Minimax algorithm.\begin{center}
    \includegraphics[width=1\textwidth ]{images/cutofftree.eps}
\end{center}
Usually, the evaluation function is a linear weighted function, such as \begin{equation}
    f(s)=\sum_{i=1}^nw_if_i(s)
\end{equation}
where $f_1\dots f_n$ are features extracted from the state $s$, designed by human experts of the considered game/domain, and $w_i\in \R$ are real weights, that can be learned automatically (with machine learning algorithms). For example, features in the tetris game could be the number of holes in the block grid, or the maximum height reached by a placed block, as shown in figure \ref{fig:tetris}.\bigskip

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth ]{images/grid_properties.eps}
    \caption{Grid's properties}
    \label{fig:tetris}
\end{figure}

This algorithm is critical since critical  aspects of the game may be cut-off by the horizon. Since we want to search \textit{as deeply as possible in a given time}, we could apply the \textit{iterative deepening search}, until the time's up, and then return the solution of the deepest completed search.\bigskip 

A better solution is called \textit{Quiescence search}, Instead of using a fixed search depth $d$, quiescence search dynamically adapts the depth to handle ''unquiet'' positions where the evaluation function's value is likely to fluctuate quickly—typically due to immediate, forcing moves like captures or checks.

For example, in Chess, if a piece exchange is underway, the search continues past the standard depth limit until a ''quiet'' state is reached, ensuring the final evaluated position is stable and doesn't miss an immediate, significant change in material or safety, thus leading to a much more accurate evaluation.

\subsection{Alpha-Beta Pruning}
There is a way to save time during the computation, we can cut-off some branches of the search for which we know at priori that will not lead to the solution. Consider the tree in figure \ref{fig:alphaPruning}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.3\textwidth ]{images/alphapruning.png}
    \caption{Let's say $n>m$}
    \label{fig:alphaPruning}
\end{figure}

THe left branch leads to a utility at least $n$, the right branch have one sub-branch with utility value $m<n$, so we can say for sure that the left tree will not provide values for the utility grater than $m$ (since it's the $Min$ turn), at this point we can discard the left branch and continue with the right one.\bigskip

We consider two additional variables $\alpha,\beta$ defined on each node during the search, such that:\begin{itemize}
    \item for each node $n$, $\alpha$ is the highest utility that search has already found for $Max$ on its path to $n$.\begin{itemize}
        \item In a $Min$ node, if one of the successors has a utility value less or equal than $\alpha$, we can stop considering $n$ and cutting it off  (pruning out its remaining successors).
    \end{itemize}
    \item We can consider a dual method for $Min$, for each node $n$, $\beta$ is the lowest utility that search has already found for $Min$ on its path to $n$.\begin{itemize}
        \item in a $Max$ node, if one of the successors has a utility value greater or equal than $\beta$, we can stop considering $n$ and cutting it off  (pruning out its remaining successors).
    \end{itemize}
\end{itemize}

%%%
 \begin{algorithm}
    \caption{AlphaBetaPruning}\label{alg:alphabetapruning}
    \begin{algorithmic}
    \Require $s$
    \State $v\leftarrow$MaxValue($s$,$-\infty$,$+\infty$)
    \State\Return an action $a\in $\texttt{Actions}($s$) yielding value $v$
    \end{algorithmic}
\end{algorithm}
 \begin{algorithm}
    \caption{MaxValue}\label{alg:ABMaxValue}
    \begin{algorithmic}
    \Require $s,\alpha,\beta$
    \If{TerminalTest($s$)}
    \State\Return $u(s)$
    \EndIf
    \State$v\leftarrow -\infty$
    \For{each $a\in $\texttt{Actions}($s$)}
    \State $v\leftarrow$ max($v$,MinValue(ChildState($s$,$a$)),$\alpha$,$\beta$)
    \State$\alpha\leftarrow\max(\alpha,v)$
    \State\textbf{if $v\ge\beta$ then return $v$}
    \EndFor 
    \State\Return $v$
    \end{algorithmic}
\end{algorithm}
  \begin{algorithm}
    \caption{MinValue}\label{alg:ABMaxValue}
    \begin{algorithmic}
    \Require $s,\alpha,\beta$
    \If{TerminalTest($s$)}
    \State\Return $u(s)$
    \EndIf
    \State$v\leftarrow +\infty$
    \For{each $a\in $\texttt{Actions}($s$)}
    \State $v\leftarrow$ min($v$,MaxValue(ChildState($s$,$a$)),$\alpha$,$\beta$)
    \State$\beta\leftarrow\min(\beta,v)$
    \State\textbf{if $v\le\alpha$ then return $v$}
    \EndFor 
    \State\Return $v$
    \end{algorithmic}
\end{algorithm}
%%%
\subsection{Monte-Carlo Tree Search}
\redText{TODO}

\chapter{Constraint Satisfaction Problems}
\begin{definition}
    A \textbf{CSP} (constraint satisfaction problem) is composed by a set of variables, each associated with it's domain, and a set of constraints over these variables, the goal is to find an assignment of variables so that every constraint is satisfied.
\end{definition}
In SuDoKu, the variables are the content of the cells, the domain of each cell is $\{1,2,\dots 9\}$, and the constraints are that, each number should appear only once in each row, column or block. Another CSP problem is the \textit{graph coloring} problem, where we should assign one of the $k$ possible color to each node, such that two adjacent nodes must have different colors (this problem is $\mathsf{NP}$-hard for $k=3$).
\section{Constraint Networks}
\begin{definition}
    A \textbf{Binary Constraint Network} is a tuple $\gamma=(V,D,C)$ where:
    \end{definition}\begin{itemize}
        \item $V=\{v_1,v_2\dots v_n\}$ is a finite set of variables. 
        \item $D=\{D_{v_1},D_{v_2}\dots D_{v_n}\}$ is a corresponding set of finite domains.
        \item $C$ is a set of binary relations that models the constraints. A relation is denoted $C_{uv}$ with $u,v$ variables in $V$. $$C_{uv}\subseteq D_u\times D_v$$If $C_{uv}\in C$ and $C_{xy}\in C$, then $\{u,v\}\ne \{x,y\}$ (no redundancy).
    \end{itemize}
    $C_{uv}$ defines the permissible combined assignments to $u$ and $v$. For example, if\begin{align}
        &u,v\in V\\
        &v'\in D_{v}\\ 
        &u' \in D_{u}\\ 
        & C_{uv}\subseteq D_{v}\times D_{u} \in C\\
        &(v',u')\notin C_{uv}
    \end{align}
the assignment $v=v', u=u'$ violates the constraints. Let's see an example, we consider the map of australia show in figure \ref{fig:australia}. We want to colorate each state with one of 3 possible colors, without having two adjacent state with the same color.\bigskip

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth ]{images/australia.png}
    \caption{Coloring Australia}
    \label{fig:australia}
\end{figure}

The variables are the states 
$$ V=\{WA,NT,SA,Q,NSW,V,T\}$$
The domain for each variable $v\in V$ is $D_v=\{red,green,blue\}$.
If all the variables have the same domain \begin{equation}
    \forall v\ne u, D_v=D_u
\end{equation}
we do an notation abuse saying that the domain of the problem is $D=\{red,green,blue\}$.
For each adjacent couple of states $u,v$, there is a constraint
$$C_{uv}=\{(d,d')\in D\times D \ : \ d\ne d'\}.$$
Constraint Networks can be extended\begin{itemize}
    \item the domains $D_v$ may be infinite or uncountable, like $D_v=\R$.
    \item the constraints may have an arity higher than 2, with relations over $k>2$ variables, like in CNF satisfiability.
\end{itemize}
A constraint may be \textbf{unary}: $C_v\subseteq D_v$, in this case the domain of a single variable is restricted, equivalently to setting $D_v=C_v$.\bigskip 

There exists CSP solvers, generic algorithms that find assignment for constraints problem, by taking in input a constraint network as the generic language to describe the problem. The core exercise in this context is to model a problem with a constraint network.
\begin{definition}
    Let $\gamma=(V,D,C)$ to be a constraint network, a \textbf{partial assignment} is a function $$a:V'\rightarrow \bigcup_{v\in V}D_v$$ where \begin{itemize}
        \item $V'\subseteq V$
        \item $a(v)\in D_v$ for all $v\in V'$
    \end{itemize}
    if $V'=V$ is a \textbf{total assignment} (or just assignment). A partial assignment assigns some variables to values from their
respective domains. A total assignment is defined on all variables.
\end{definition}
\begin{definition}
    Let $\gamma=(V,D,C)$ to be a constraint network and let $a$ to be a partial assignment, we say that $a$ is \textbf{inconsistent} if there exists variables $u,v\in V$ such that
\end{definition}
\begin{itemize}
        \item $a$ is defined on $u$ and $v$
        \item $C_{uv}\in C$
        \item $(a(u),a(v))\notin C_{uv}$.
    \end{itemize}
If a partial assignment is not inconsistent, is \textbf{consistent}. An assignment $a$ is a \textbf{solution} if it's total and consistent. A constraint network $\gamma$ is \textbf{solvable} if there exists a solution $a$, otherwise is \textbf{unsolvable}.\begin{center}
    \includegraphics[width=0.8\textwidth ]{images/australia2.png}
\end{center}
\begin{definition}
    Let $\gamma=(V,D,C)$ to be a constraint network, and let $a$ to be a partial assignment. $a$ can be \textbf{extended} to a solution if there exists a solution $a'$ that agrees with $a$ on the variables where $a$ is defined.
\end{definition}
\begin{itemize}
    \item the domain of $a$ is $V'\subset V$
    \item for each $v\in V'$, we have that $a(v)=a'(v)$.
\end{itemize}
\begin{proposition}
    if $a$ can be extended to a solution, then it's consistent (the opposite doesn't necessary holds).
\end{proposition}
Given a constraint network $\gamma=(V,D,C)$, if $n=|V|$ and $\forall D_v\in D$, $|D_v|=k$, the number of total assignments is $k^n$. There are at most $n^2$ constraints, each constraint have size at most $k^2$, the number of assignments is \textit{exponentially bigger} than the size of $\gamma$.\begin{theorem}
    The problem to decide if a constraint network $\gamma=(V,D,C)$, is solvable is  $\mathsf{NP}$-complete.
\end{theorem}
\section{Naive Backtracking}
The following algorithm is simple and used to find a solution for a constraint network. The method is recursive, the first step given assignment $a$ is the empty assignment.
 \begin{algorithm}
    \caption{NaiveBacktracking}\label{alg:NaiveBacktracking}
    \begin{algorithmic}
    \Require $\gamma=(V,D,C),a$
    \If{$a$ is inconsistent} \State\Return inconsistent\EndIf
    \If{$a$ is a total assignment} \State\Return $a$\EndIf
    \State let $V'$ to be the domain of $a$
    \State select \color{blue} some  \color{black} $v\in V-V'$ (select a variable for which $a$ is not defined)
    \For{ each $d\in D_v$ in \color{blue} some order \color{black}}
    \State $a'=a\cup \{v=d\}$
    \State $a''=$NaiveBacktracking($\gamma$,$a'$)
    \If{$a''$ is not inconsistent} \State\Return $a''$\EndIf
    \EndFor
    \State\Return inconsistent
    \end{algorithmic}
\end{algorithm}
With \textit{Backtracking}, we mean the action of recursively instantiate variables one-by-one, backing up out
of a search branch if the current partial assignment is already inconsistent.
Note that in the algorithm \ref{alg:NaiveBacktracking} we are iterating the possible values in $D_v$ in a specific \color{blue} order \color{black} that is not described yet, and is not described which variable are we picking at each recursion step.\bigskip 

\noindent The Naive Backtracking algorithm is\begin{itemize}
    \item simple to implement 
    \item much more efficient than enumerating all possible assignment
    \item complete, if there is a solution, backtracking
will find it.
\end{itemize}
This algorithm can't predict if an assignment $a$ can't be extended to a solution unless $a$ is already inconsistent.
\subsection{About the Order}
The ordering in the for loop of algorithm \ref{alg:NaiveBacktracking} can  influences the search space size. If no solution exists below current node, the order doesn't matter, the algorithm will search the whole sub-tree anyway, otherwise, if a solution does exist below current node, by choosing the ''correct'' value, then no backtracking is needed.\bigskip 

A common strategy is to consider the variable ''most constrained'', at each recursion step, we pick the variable $v$ that minimize the size of the set\begin{equation}
    \{d\in D_v \ : \ a\cup\{v=d\} \text{ is consistent }\}
\end{equation}
by choosing a most constrained variable $v$ first, we reduce the
branching factor (number of sub-trees generated for $v$) and thus reduce
the size of our search tree.\bigskip 

Another common heuristic is to choose the variable that is the ''most constraining'', we choose $v$ that maximize the size of the set\begin{equation}
    \{u\in V \ : \ a(u)\text{ is undefined }\land C_{uv}\in C \}
\end{equation}
by choosing a most constraining variable first, we detect
inconsistencies earlier on and thus reduce the size of our search tree.\bigskip 

For the order of the value in the for loop, we can choose the least constraining value first, for a variable $v$, we choose the value $d\in D_v$ that minimize the size of the set\begin{equation}
    \{d' \ : \ d'\in D_u\land a(u)\text{ is undefined }\land C_{uv}\in C\land (d',d)\notin C_{uv}\}
\end{equation}
by choosing a least constraining value first, we increase the chances
to not rule out the solutions below the current node. 
\section{Inference}
Given a constraint networks, we would like to obtain an equivalent network with more constraint, without losing any feasible solution, in such case, we are giving a tighter description of the problem, and we obtain a network with a smaller number of
consistent partial assignments. Already known constraints may implies additional (unary or binary) constraints.\begin{definition}
    Let $\gamma=(V,D,C)$ and $\gamma'=(V,D',C')$ to be two constraint networks, sharing the same set of variables. We say that $\gamma$ are \textbf{equivalent} to $\gamma'$, and we denote $$ \gamma\equiv\gamma' $$
    if every solution (assignment of variables) of $\gamma$ is a solution of $\gamma'$ and vice versa.
\end{definition}
\begin{definition}
    Let $\gamma=(V,D,C)$ and $\gamma'=(V,D',C')$ to be two constraint networks, sharing the same set of variables. We say that $\gamma'$ is \textbf{tighter} than $\gamma$ and we denote $$ \gamma'\sqsubseteq \gamma $$
    if
\begin{itemize}
    \item $\forall v\in V, \ \ D'_v\subseteq D_v$
    \item $\forall u,v \text{ such that }u\ne v$ either $C_{uv}\notin C$ or $C'_{uv}\subseteq C_{uv}$.
\end{itemize}\end{definition}
If at least one of these inclusions is strict, we say that  $\gamma'$ is \textbf{strictly tighter} than $\gamma$: $$ \gamma'\sqsubset \gamma. $$\begin{theorem}
    Let $\gamma$ and $\gamma'$ to be two constraint networks, if\begin{itemize}
        \item $\gamma'\equiv\gamma$
        \item $\gamma'\sqsubset\gamma$
    \end{itemize}
    then $\gamma'$ has the same solutions as $\gamma$ but fewer consistent partial
assignments than $\gamma$. In such case, $\gamma'$ is a \textbf{better encoding} of the problem.
\end{theorem}
We define \textit{inference} the procedure to derive a tighter equivalent network. We want to use it to define an algorithm that finds a solution for a constraint network. We will incorporate inference in the  backtracking algorithm, at every recursive call, a complex inference procedure leads to\begin{itemize}
    \item a smaller  number of search nodes 
    \item larger runtime needed at each node
\end{itemize} 
We encode partial assignments as unary constraints\begin{itemize}
    \item if $a(v)=d$
    \item we set the constraint $D_v=\{d\}$
\end{itemize}

\begin{algorithm}
    \caption{BacktrackingWithInference}\label{alg:BacktrackingWithInference}
    \begin{algorithmic}
    \Require $\gamma=(V,D,C),a$
    \If{$a$ is inconsistent} \State\Return inconsistent\EndIf
    \If{$a$ is a total assignment} \State\Return $a$\EndIf
    \State $\gamma'=$ copy of $\gamma$
    \State \color{orange}$\gamma'=$ Inference$(\gamma')$\color{black}
    \State\textbf{If} $\exists v \ : \ D'_v=\emptyset$ \textbf{ then return } inconsistent
    \State let $V'$ to be the domain of $a$
    \State select \color{blue} some  \color{black} $v\in V-V'$ (select a variable for which $a$ is not defined)
    \For{ each $d\in D'_v$ in \color{blue} some order \color{black}}
    \State $a'=a\cup \{v=d\}$
    \State $D'_v=\{d\}$
    \State $a''=$NaiveBacktracking($\gamma'$,$a'$)
    \If{$a''$ is not inconsistent} \State\Return $a''$\EndIf
    \EndFor
    \State\Return inconsistent
    \end{algorithmic}
\end{algorithm}
With the function \color{orange}Inference \color{black} we denote any procedure deriving a tighter equivalent network. One simple inference procedure called \textit{forward checking} is described in algorithm \ref{alg:FWCH}.\bigskip 

\begin{algorithm}
    \caption{Forward Checking}\label{alg:FWCH}
    \begin{algorithmic}
    \Require $\gamma,a$
    \For{ each $v$ where $a(v)=d'$ is defined }
    \For{ each $u$ where $a(u)$ is undefined and $C_{uv}\in C$}
    \State $D_u=\{d \ | \ d\in D_u, (d,d')\in C_{uv}\}$
    \EndFor 
    \EndFor 
     \State\Return $\gamma$
    \end{algorithmic}
\end{algorithm}

The forward checking algorithm tightens the constraints without ruling out any solutions, it guarantees to deliver an equivalent network. The computation can be done incrementally, instead of performing the first loop, we may run only the second loop every time a new assignment $a(v)=d'$ is added.\begin{itemize}
    \item The forward checking procedure is cheap and useful 
    \item In rare situations is not a good idea to use it, there are stronger inference methods, but in many cases, forward checking is the best choice.
\end{itemize}

\subsection{Arc Consistency for Stronger Inference}
We now describe a properties between two variables that can be used to describe a stronger inference method than the forward checking.\begin{definition}
    Let $\gamma=(V,D,C)$ to be a constraint network, a variable $u\in V$ is \textbf{arc consistent} to another variable $v\in V$ if \begin{itemize}
            \item either $C_{uv}\notin  C$ 
            \item or for every $d\in D_u$ there exists $d'\in D_v$ such that $(d,d')\in C_{uv}$.
    \end{itemize}
\end{definition}
When two variables in a constraint network are arc consistent, it means that the domain  of each variable contains at least one value that is compatible with at least one value in the domain of the other variable, according to the constraint between them.

As an example, let's say the variable $v$ describes the color of your shirt, that can be red, blue, yellow or green, and the variable $u$ define the color of your pants, that can be black, white, gray or brown. The constraint is that the shirt and the pants cannot be the same color, the variable $v$ is  arc consistent to $u$ because for every shirt color you can find a pant color and vice versa.\begin{quote}
    \textbf{Note}: Arc consistency is a non commutative relation, if $v$ is  arc consistent to $u$, it's not implied directly that $u$ is  arc consistent to $v$.
\end{quote}
\begin{definition}
    Let $\gamma=(V,D,C)$ to be a constraint network, if every variable $u\in V$ is arc consistent relative to every other variable $v\in V$, then we say that the network $\gamma$ is \textbf{arc consistent}.
\end{definition}
We can define an inference procedure that aims to make the network $\gamma$ arc consistent, by removing variable domain values. Let's say that AC($\gamma$) is a procedure that makes $\gamma$ arc consistent, in such case:\begin{itemize}
    \item is guaranteed that AC($\gamma$) returns an equivalent network.
    \item AC($\gamma$) is more powerful (more general) tha forward checking, in details:\begin{equation}
        \text{AC}(\gamma)\sqsubseteq \text{ForwardChecking}(\gamma)
    \end{equation}
\end{itemize}
We define the following subroutine:

\begin{algorithm}
    \caption{Revise}\label{alg:revise}
    \begin{algorithmic}
    \Require $\gamma,u\in V,v\in V$
    \For{ each $d\in D_u$}
    \If{$\not\exists d'\in D_v$ such that $(d,d')\in C_{uv}$}
    \State $D_u=D_u\backslash\{d\}$
    \EndIf
    \EndFor 
     \State\Return $\gamma$
    \end{algorithmic}
\end{algorithm}

We apply pairwise Revisions in algorithm \ref{alg:AC1} and define the first version of the inference procedure that makes a constraint network $\gamma$ arc consistent.\bigskip

\begin{algorithm}
    \caption{AC\_1}\label{alg:AC1}
    \begin{algorithmic}
    \Require $\gamma$
    changes = False
    \While{changes = False }
    \For{ each $C_{uv}\in C$}
    \State\textbf{If } $D_u$ after applying Revise($\gamma,u,v$ reduces) \textbf{ then } changes = True
    \State\textbf{If } $D_v$ after applying Revise($\gamma,v,u$ reduces) \textbf{ then } changes = True
    \EndFor
    \EndWhile
    \end{algorithmic}
\end{algorithm}

It's not hard to show that this enforce arc consistency. If there are $n$ variables, $m$ constraints and $k$ is the cardinality of the biggest domain, then the time complexity is $O(mk^2\cdot nk)$, this algorithm performs redundant computation since $u$ and $v$ are revised even if their domains
haven't changed since the last time.\bigskip 

\noindent There is another version of the arc consistency procedure, described in algorithm \ref{alg:AC3}. At any moment during the while loop, if $(u,v)\notin M$ then $u$ is arc consistent to $v$.

\begin{algorithm}
    \caption{AC\_3}\label{alg:AC3}
    \begin{algorithmic}
    \Require $\gamma$
    \State $M=\emptyset$
     \For{ each $C_{uv}\in C$}
     \State $M=M\cup\{(u,v),(v,u)\}$
     \EndFor
      \While{$M\ne\emptyset$ }
      \State remove any element $(u,v)$ from $M$
      \State Revise($\gamma,u,v$)
      \If{ $D_u$ has changed in the call Revise}
      \For{ each $C_{wu}\in C$ where $w\ne v$}
      \State $M=M\cup\{(w,u)\}$
       \EndFor
      \EndIf 
      \EndWhile
    \end{algorithmic}
\end{algorithm}

In the algorithm, we check the constraints where $w\ne v$ because $v$ is the reason why $D_u$ just changed, if $v$ was arc consistent to $u$ before, that continues to hold, the values just removed from $D_u$ did not match any values from $D_v$ anyway.
\begin{theorem}
    Let $\gamma$ to be a constraint network with $m$ constraint and maximal domain size $k$. The algorithm AC3 showed in \ref{alg:AC3} have a time complexity in $O(mk^3)$.
\end{theorem}
\textit{Proof}: Each Revise call takes $O(k^2)$, it is sufficient to prove that at most $O(mk)$ calls to revise are made. 
The number of calls to Revise is the number of iterations of the
while-loop, which is at most the number of insertions into $M$. \begin{itemize}
    \item Consider
    any constraint $C_{uv}$
    \item two variables pairs corresponding to the constraint $C_{uv}$ are inserted into $M$ in the first for loop
    \item then
beforehand the domain of either $u $ or $v$ was reduced, which happens at
most $2k$ times 
\item thus we have $O(k)$ insertions per constraint, and $O(mk)$ insertions overall.\hfill$\blacksquare$
\end{itemize}

\section{Decomposition}
If $\gamma$ is a network with $n$ variables and maximal domain size $k$, we may search a feasible assignment by checking among all the possible ones, this will require to check $O(k^n)$ assignments in the worst case. We saw how the inference help to shrink the search space, another useful methods is called \textit{decomposition}.\begin{quote}
    \textbf{decomposition}: exploit the structure of a network to decompose it into smaller (and easier to solve) independent networks.
\end{quote}
\begin{definition}
    Given a constraint network $\gamma=(V,D,C)$, the \textbf{constraint graph} of $\gamma$ is the undirected graph, where the vertices are the variables $V$ and there are an edge $(u,v)$ if $C_{uv}\in C $.
\end{definition}
For example, the graph for the ''coloring Australia'' problem shown in figure \ref{fig:australia} is the following:\begin{center}
    \includegraphics[width=0.4\textwidth ]{images/australia_graph.png}
\end{center}
Note how this graph is composed by two connected sub-graph, since the variable $T$ (Tasmania) is disconnected from the other variables. The idea is to separate a network in function of his connected sub-graphs.  
\begin{theorem}
    Let $\gamma=(V,D,C)$ to be a constraint network. Let $G$ to be the constraint graph, we denote $\mathcal V(G)$ the vertices of $G$ and $\mathcal E(G)$ the edges of $G$, we know that $\mathcal V(G)=V $. We consider a partition of $\mathcal V(G)$\begin{equation}
        \mathcal V(G)=\bigcup_i V_i
    \end{equation}
    such that each $V_i$ is connected and $$\begin{cases}
        v\in V_i\\ 
        u\in V_j\\ 
        V_i\ne V_j
    \end{cases}\implies\begin{matrix}
     (u,v)\notin \mathcal E(G)\\ 
      (v,u)\notin \mathcal E(G)
    \end{matrix} \implies C_{uv}\notin C$$
    so the components of the partition are disconnected from each other. Let $a_i$ to be an feasible assignment (solution) for the variables $V_i$, then \begin{equation}
        a=\cup_i a_i 
    \end{equation}
    is a solution to $\gamma$.
\end{theorem}
The theorem states that if two parts of $\gamma$ are not connected (in term of his constraint graph), then they are independent and we can find separately a solution for each connected component.\bigskip 

\begin{theorem}
    Let $\gamma=(V,D,C)$ to be a constraint network with $n$ variables and maximal domain size $k$. Let $G$ to be the constraint graph, if $G$ is acyclic, we can find a solution for $\gamma$ (or prove that $\gamma$ is inconsistent) with a time complexity of $O(nk^2)$.
\end{theorem}
The procedure is the following (assuming that a constraint network is connected, if not, we can apply the procedure to each connected component).\begin{enumerate}
    \item We consider the constraint graph of $\gamma$, denoted $G$. We choose an arbitrary node $v\in \mathcal V(G)=V$, and we consider the BFS spanning tree $G'$ starting from $v$. 
    \item $G'$ is a directed graph, we consider a topological ordering $v_1\dots, v_n$ of the variables.
    \item we iterate through the variables $v_i$ in the opposite order respect to the topological ordering: for each $i=n,n-1,n-2,\dots, 3,2$ (except for $v_1$)\begin{enumerate}
        \item we perform the procedure Revise($\gamma$, parent($v_i$), $v_i$)
        \item if $D_{\text{parent($v_i$)}}\ne \emptyset$, return inconsistent
    \end{enumerate}
    at this point,  every variable is arc consistent relative to its children
    \item We perform the algorithm \ref{alg:BacktrackingWithInference} using the variable order $v_1,v_2\dots , v_n$.
\end{enumerate}
\subsection{Cutsets}
\begin{definition}
    Let $\gamma=(V,D,C)$ to be a constraint network. Let $G$ to be the constraint graph, a \textbf{cutset} $V_0\subseteq V$ is a set such that, the graph $G'$ where $\mathcal V(G')=V\backslash V_0$ is acyclic.
\end{definition}
A cutset is a subset of variables removing which renders the constraint
graph acyclic. The graph of the ''coloring Australia'' problem is \textit{almost acyclic} since, removing $SA$ will make the graph a tree.\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/australia_graph2.png}
\end{center}
The following procedure use cutsets to find a solution for $\gamma$.
\begin{center}
    \includegraphics[width=0.8\textwidth ]{images/cutset.png}\bigskip

    \redText{TODO: Riscrivere questo algoritmo, l'identazione è ambigua}
\end{center}
The forward checking operation is required to ensure that $a\cup a'$ is consistent in $\gamma$, the runtime is exponential in $V_0$. It is true that finding an optimal cutset for a graph is $\mathsf{NP}$-hard, but practical approximated procedures exists.
\end{document}